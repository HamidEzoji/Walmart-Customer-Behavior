# -*- coding: utf-8 -*-
"""Final version.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AT8hN876ZyGkLADUwYzWJuuqLsKYnlnx
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.cluster import KMeans


# Load dataset
df = pd.read_csv("https://drive.google.com/uc?export=download&id=1CFh9rxshWQV7MqYLFn753phj_65UiiyJ", parse_dates=["Purchase_Date"])
df['Purchase_Date'] = pd.to_datetime(df['Purchase_Date'], errors='coerce')
print(df.head())

# Checking the number of rows and columns
print(f"Dataset Shape: {df.shape}")

# Column types & non-null counts
print("\nColumn Information:")
df.info()

# Missing values per column
print("\nMissing Values:")
print(df.isnull().sum())

# Check for duplicate entries
print(f"\nDuplicate Rows: {df.duplicated().sum()}")
# Drop duplicates
df = df.drop_duplicates()

# Detect hidden/null-like strings
for col in df.columns:
    print(f"{col} unique values:", df[col].unique()[:10])

for col in df.select_dtypes(include='object'):
    print(f"{col}: {df[col].nunique()} unique values")
    print(df[col].value_counts(dropna=False))
    print("\n")

# Check for hidden missing value placeholders
for col in df.columns:
    print(col, df[col].isin(['N/A', 'null', '', '-']).sum())

# Plot histograms
df[['Age', 'Purchase_Amount', 'Rating']].hist(bins=20, figsize=(12, 5))
plt.tight_layout()
plt.show()

# Boxplots for outliers
for col in ['Age', 'Purchase_Amount', 'Rating']:
    plt.figure(figsize=(6, 2))
    sns.boxplot(data=df, x=col)
    plt.title(f'Boxplot of {col}')
    plt.show()

categorical_cols = ['Gender', 'Category', 'Product_Name', 'Payment_Method', 'Discount_Applied', 'Repeat_Customer']

for col in categorical_cols:
    plt.figure(figsize=(8, 4))
    sns.countplot(data=df, x=col, order=df[col].value_counts().index)
    plt.xticks(rotation=45)
    plt.title(f'Distribution of {col}')
    plt.tight_layout()
    plt.show()

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Seasonality and Temporal Patterns
df['month'] = df['Purchase_Date'].dt.month
df['day_of_week'] = df['Purchase_Date'].dt.day_name()

# Purchases by month
plt.figure(figsize=(10,6))
sns.countplot(data=df, x='month')
plt.title('Monthly Purchase Trends')
plt.show()

# Purchases by day of week
plt.figure(figsize=(10,6))
sns.countplot(
    data=df,
    x='day_of_week',
    order=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']
)
plt.title('Day-of-Week Purchase Trends')
plt.show()

#Purpose: This boxplot visualizes the distribution of Age across Repeat Customer status,
#comparing the age of repeat customers (e.g., "Yes") versus non-repeat customers (e.g., "No").
sns.boxplot(data=df, x='Repeat_Customer', y='Age')
plt.title('Age by Repeat Customer Status')
plt.show()

#Purpose: This boxplot visualizes the distribution of Purchase Amount across Repeat Customer status,
#comparing how much repeat customers spend versus non-repeat customers.
sns.boxplot(data=df, x='Repeat_Customer', y='Purchase_Amount')
plt.title('Purchase Amount by Repeat Status')
plt.show()

# Category vs Repeat Customer
#Purpose: This plot shows the Repeat Customer rate by Product Category.
#It compares the number of repeat and non-repeat customers across various product categories.
sns.countplot(data=df, x='Category', hue='Repeat_Customer')
plt.title('Repeat Customer Rate by Product Category')
plt.xticks(rotation=45)
plt.show()

# Gender vs Repeat
#Purpose: This plot shows the Repeat Customer rate by Gender,
#comparing the number of repeat and non-repeat customers for different genders.
sns.countplot(data=df, x='Gender', hue='Repeat_Customer')
plt.title('Repeat Customer by Gender')
plt.show()

# First make sure date column is datetime type
df['Purchase_Date'] = pd.to_datetime(df['Purchase_Date'])

# Create new time features
df['day_of_week'] = df['Purchase_Date'].dt.dayofweek  # Monday=0, Sunday=6
df['month'] = df['Purchase_Date'].dt.month
df['day'] = df['Purchase_Date'].dt.day
df['year'] = df['Purchase_Date'].dt.year

# Daily Purchases Trend
daily_purchases = df.groupby('Purchase_Date').size()

plt.figure(figsize=(15,6))
daily_purchases.plot()
plt.title('Daily Purchases Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Purchases')
plt.show()

# Weekly Pattern (Purchases by Day of the Week)
plt.figure(figsize=(10,5))
sns.countplot(data=df, x='day_of_week', order=[0,1,2,3,4,5,6], palette='Blues')
plt.title('Purchases by Day of the Week')
plt.xlabel('Day of the Week (0=Monday)')
plt.ylabel('Number of Purchases')
plt.xticks(ticks=[0,1,2,3,4,5,6], labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
plt.show()

# Monthly/Seasonal Trend (Purchases by Month)
plt.figure(figsize=(10,5))
sns.countplot(data=df, x='month', palette='Purples')
plt.title('Purchases by Month')
plt.xlabel('Month')
plt.ylabel('Number of Purchases')
plt.show()

# Heatmap (Weekday vs Month) - Advanced!
pivot_table = df.pivot_table(index=df['month'], columns=df['day_of_week'], values='Purchase_Amount', aggfunc='count')

plt.figure(figsize=(12,8))
sns.heatmap(pivot_table, cmap='coolwarm', annot=True, fmt='.0f')
plt.title('Heatmap of Purchases by Month and Day of Week')
plt.xlabel('Day of the Week (0=Monday)')
plt.ylabel('Month')
plt.show()

# Plotting purchase count and total sales side-by-side

# Group by 'Category' and aggregate purchase count and total sales amount
combined = df.groupby('Category').agg(
    purchase_count=('Purchase_Date', 'count'),  # count purchases
    Purchase_Amount=('Purchase_Amount', 'sum')   # sum purchase amounts
).reset_index()

# Select top 10 categories
top_10_combined = combined.head(10)

# Set position for each category
x = np.arange(len(top_10_combined['Category']))  # the label locations
width = 0.4  # width of the bars

# Create subplots
fig, ax1 = plt.subplots(figsize=(14,7))

# Bar for purchase count
rects1 = ax1.bar(x - width/2, top_10_combined['purchase_count'], width, label='Number of Purchases', color='skyblue')

# Create another y-axis for total sales
ax2 = ax1.twinx()

# Bar for total sales
rects2 = ax2.bar(x + width/2, top_10_combined['Purchase_Amount'], width, label='Total Sales Amount', color='lightgreen')

# Titles and labels
ax1.set_xlabel('Product Category')
ax1.set_ylabel('Number of Purchases', color='skyblue')
ax2.set_ylabel('Total Sales Amount ($)', color='lightgreen')
plt.title('Top 10 Product Categories: Number of Purchases vs Total Sales')

# Set x-ticks
ax1.set_xticks(x)
ax1.set_xticklabels(top_10_combined['Category'], rotation=45, ha='right')

# Legends
fig.legend(loc="upper right", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)

# Add data labels
for rect in rects1:
    height = rect.get_height()
    ax1.annotate('{}'.format(int(height)),
                 xy=(rect.get_x() + rect.get_width() / 2, height),
                 xytext=(0, 5),  # 5 points vertical offset
                 textcoords="offset points",
                 ha='center', va='bottom', fontsize=8, color='blue')

for rect in rects2:
    height = rect.get_height()
    ax2.annotate('${:,.0f}'.format(height),
                 xy=(rect.get_x() + rect.get_width() / 2, height),
                 xytext=(0, 5),
                 textcoords="offset points",
                 ha='center', va='bottom', fontsize=8, color='green')

plt.tight_layout()
plt.show()

def prepare_features(df_cust, encode_high_card=False, df_full=None):
    X = df_cust.copy()
    X['repeat']      = (X['visits'] > 1).astype(int)
    X['recent_high'] = (X['recency_days'] < 30).astype(int)
    # keep City here so we can encode it
    X = X.drop(columns=['Customer_ID','first_purchase','last_purchase'])
    if encode_high_card and df_full is not None:
        city_counts  = df_full['City'].value_counts()
        X['city_freq'] = X['City'].map(city_counts)
        X = X.drop(columns=['City'])
    return X

# Convert to numeric where needed
df['Discount_Applied'] = df['Discount_Applied'].map({'Yes': 1, 'No': 0})
df['Repeat_Customer'] = df['Repeat_Customer'].map({'Yes': 1, 'No': 0})

# Now safe to aggregate
customer_df = df.groupby("Customer_ID").agg(
    last_purchase=("Purchase_Date", "max"),
    visits=("Purchase_Date", "count"),
    total_spent=("Purchase_Amount", "sum"),
    avg_rating=("Rating", "mean"),
    first_purchase=("Purchase_Date", "min"),
    n_categories=("Category", pd.Series.nunique),
    discount_rate=("Discount_Applied", "mean"),
    repeat=("Repeat_Customer", "max")
).reset_index()

# Additional features
customer_df["recency_days"] = (df["Purchase_Date"].max() - customer_df["last_purchase"]).dt.days
customer_df["frequency"] = customer_df["visits"] / ((customer_df["last_purchase"] - customer_df["first_purchase"]).dt.days + 1)
customer_df["recent_high"] = (customer_df["total_spent"] > customer_df["total_spent"].median()).astype(int)

# — Customer Segmentation with KMeans (Safe Copy) —
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline

# Define the features used for clustering
cluster_features = [
    'visits',
    'recency_days',
    'frequency',
    'total_spent',
    'avg_rating',
    'n_categories'
]

# Create a separate copy to protect customer_df
segmentation_df = customer_df.copy()

# Drop rows with missing values only for segmentation
segmentation_df = segmentation_df.dropna(subset=cluster_features)

# Build the clustering pipeline
cluster_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('kmeans', KMeans(n_clusters=4, random_state=42))
])

# Apply clustering and add the labels
segmentation_df['cluster'] = cluster_pipeline.fit_predict(segmentation_df[cluster_features])

# Show the distribution of customers per cluster
print("Cluster counts:\n", segmentation_df['cluster'].value_counts())

# ——— Classification on Enriched RFM Features ———
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report

# 1) Build X, y from customer_df
feature_cols = [
    'visits',
    'recency_days',
    'frequency',
    'recent_high',
    'n_categories',
    'discount_rate',
    'avg_rating'
]
X = customer_df[feature_cols]
y = customer_df['repeat'].astype(int)

print("Class balance:\n", y.value_counts())

# 2) Preprocessor for numeric features
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), feature_cols)
], remainder='drop')

# 3) Define & wrap models
models = {
    'LogisticRegression': LogisticRegression(class_weight='balanced', max_iter=1000),
    'RandomForest'      : RandomForestClassifier(class_weight='balanced', random_state=42)
}
def make_pipe(m): return Pipeline([('prep', preprocessor), ('clf', m)])

# 4) Stratified split
X_tr, X_test, y_tr, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
X_train, X_val, y_train, y_val = train_test_split(
    X_tr, y_tr, test_size=0.25, random_state=42, stratify=y_tr
)

print("Train balance:\n", y_train.value_counts())
print("Val   balance:\n", y_val.value_counts())

# 5) Fit & evaluate on validation
for name, mdl in models.items():
    pipe = make_pipe(mdl)
    pipe.fit(X_train, y_train)
    p_val = pipe.predict_proba(X_val)[:,1]
    print(f"{name} → Val AUC: {roc_auc_score(y_val,p_val):.3f}, Acc: {pipe.score(X_val,y_val):.3f}")

# 6) (Optional) Hyperparameter tuning example
from sklearn.model_selection import GridSearchCV
param_grid = {'clf__n_estimators': [50,100], 'clf__max_depth': [5,10,None]}
gs = GridSearchCV(make_pipe(models['RandomForest']), param_grid, cv=3)
gs.fit(X_train, y_train)
print("Best RF params:", gs.best_params_)
print("RF Test AUC:", roc_auc_score(y_test, gs.predict_proba(X_test)[:,1]))
print(classification_report(y_test, gs.predict(X_test)))

# ——— Regression to Predict Total Spend ———
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# 1) Build X,y without leaking total_spent
reg_y = customer_df['total_spent']
reg_X = customer_df[[
    'visits',
    'recency_days',
    'frequency',
    'recent_high'
]]

# 2) Split
Xr_tr, Xr_te, yr_tr, yr_te = train_test_split(
    reg_X, reg_y, test_size=0.2, random_state=42
)

# 3) Pipeline
reg_pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('reg',    RandomForestRegressor(random_state=42))
])

# 4) Fit & evaluate
reg_pipe.fit(Xr_tr, yr_tr)
y_pred = reg_pipe.predict(Xr_te)
print("MAE:", mean_absolute_error(yr_te, y_pred))
print("R² :", r2_score(yr_te, y_pred))

# ——— Customer‐Level Aggregation (RFM + Repeat Flag) ———

# 1a) Ensure Purchase_Date is datetime
df['Purchase_Date'] = pd.to_datetime(df['Purchase_Date'])

# 1b) First purchase per customer (for feature engineering)
first_df = (
    df
    .sort_values('Purchase_Date')
    .drop_duplicates('Customer_ID', keep='first')
    .loc[:, [
        'Customer_ID','Purchase_Date','Age','Gender','City',
        'Category','Purchase_Amount','Discount_Applied','Rating','Payment_Method'
    ]]
    .reset_index(drop=True)
)

# 1c) Aggregate the provided Repeat_Customer flag to get “ever repeat”
repeat_df = (
    df
    .assign(is_repeat = lambda d: (d['Repeat_Customer']=='Yes').astype(int))
    .groupby('Customer_ID')['is_repeat']
    .max()              # 1 if any purchase was a repeat
    .reset_index()
    .rename(columns={'is_repeat':'repeat_ever'})
)

# 1d) Merge the label back onto first_df
first_df = first_df.merge(repeat_df, on='Customer_ID', how='left')

# Sanity check
print("Total customers:    ", first_df.shape[0])
print("Ever-repeat count:  ", first_df['repeat_ever'].sum())
print("Non-repeat count:   ", (first_df['repeat_ever']==0).sum())

# — 2. Feature engineering on first purchases —
first = first_df.copy()

# 2a) Temporal features from the purchase date
first['month']     = first['Purchase_Date'].dt.month
first['weekday']   = first['Purchase_Date'].dt.day_name()

# 2b) Flag and code features
first['discount_flag'] = (first['Discount_Applied'] == 'Yes').astype(int)
first['gender_code']   = first['Gender'].map({'Male':0, 'Female':1}).fillna(2).astype(int)

# 2c) Define the feature columns and build X_first, y_first
feature_cols = [
    'Age',
    'Purchase_Amount',
    'Rating',
    'month',
    'weekday',
    'discount_flag',
    'gender_code',
    'Category',
    'Payment_Method'
]
X_first = first[feature_cols]
y_first = first['repeat_ever']

# Quick check
print("X_first shape:", X_first.shape)
print("Feature columns:", feature_cols)
print("Target distribution:\n", y_first.value_counts())

from sklearn.model_selection import train_test_split

# — 3. Stratified random train/test split —
X_train, X_test, y_train, y_test = train_test_split(
    X_first, y_first,
    test_size=0.20,
    random_state=42,
    stratify=y_first
)

# Quick sanity check
print("Train size:", X_train.shape[0], "Test size:", X_test.shape[0])
print("Train distribution:\n", y_train.value_counts())
print("Test distribution:\n", y_test.value_counts())

# — 4. SMOTE + XGB classification pipeline on RFM features —
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, classification_report

# 4a) Define exactly the columns present in customer_df
feature_cols = [
    'visits',
    'recency_days',
    'frequency',
    'recent_high',
    'n_categories',
    'discount_rate',
    'avg_rating'
]
X = customer_df[feature_cols]
y = customer_df['repeat'].astype(int)

# 4b) Stratified train/test split to preserve both classes
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Train class balance:\n", y_train.value_counts())
print("Test class balance:\n", y_test.value_counts())

# 4c) Build pipeline: scale → SMOTE → XGBoost
pipe = ImbPipeline([
    ('scale', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('clf', XGBClassifier(
        random_state=42,
        learning_rate=0.05,
        max_depth=4,
        n_estimators=100,
        use_label_encoder=False,
        eval_metric='logloss'
    ))
])

# 4d) Fit & evaluate
pipe.fit(X_train, y_train)
y_pred_proba = pipe.predict_proba(X_test)[:, 1]

print("Test AUC:", roc_auc_score(y_test, y_pred_proba))
print(classification_report(y_test, y_pred_proba >= 0.5))